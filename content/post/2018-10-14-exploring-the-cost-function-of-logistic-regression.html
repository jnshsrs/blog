---
title: Exploring the Cost Function of Logistic Regression
author: Jens Hüsers
date: '2018-10-14'
slug: exploring-the-cost-function-of-logistic-regression
categories:
  - regression
  - Logistic-regression
  - R
  - Cost-function
tags: [regression, cost-function, logistic-regression, data-visualisation]
summary: "Create a beautifully simple personal or academic website in under 10 minutes."
header:
  image: "headers/getting-started.png"
  caption: "Image credit: [**Academic**](https://github.com/gcushen/hugo-academic/)"
---



<script src="//yihui.name/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>This post is about exploring the cost function and its connection to the logistic regression function.</p>
<pre class="r"><code>knitr::opts_chunk$set(echo = TRUE, warning=F, message=FALSE, fig.align=&#39;center&#39;)
library(tidyverse)
library(plotly)
library(latex2exp)</code></pre>
<p>I want to get a deeper understanding of the connection between the logistic regresssion and its cost function. Therefore I created a function in R and conducted a grid approximation with this function. The results are presented below.</p>
<p>The logistic regression can be applied to data where the dependent variable is coded binary where the referent class is coded as a 1 and as 0 otherwise. We can model the relationship of this binary outcome variable with metric and categorical predictor variables. As a result, we can then compute the predicted probability that a datapoint is a member of the referent class coded as 1.</p>
<p>The logistic regression model with an intercept and one depentent variable is as follows:</p>
<p><span class="math display">\[\log(\frac{p(x)}{1 - p(x)}) = \beta_0 + \beta_1x_i\]</span></p>
<p>This formula models the Logarithm of the Odds-Ratio. This is the logistic regression model with an intercept and one predictor variable. To find the <span class="math inline">\(\beta\)</span>-Coefficients that fit the data best we optimize the following cost-function, the Log-Likelihood function.</p>
<p><span class="math display">\[Cost(p(x), y) = \frac{1}{n}\sum{-y_i \log(p(x_i) + (1 - y_i)\log(p(x_i)))}\]</span></p>
<p>Since the linear equation of logistic regression predicts the Logit, this equation can be rearranged to get a prediction of <span class="math inline">\(p(x)\)</span> which is basically the probability that a observation belongs to the reference group:</p>
<p><span class="math display">\[p(x) = \frac{1}{(1 + e^{-(\beta_0 + \beta_1x_i)})}\]</span></p>
<pre class="r"><code># Logistic function to predict p(x)
logistic_function &lt;- function(D, b) {
  1 / (1 + (1 / exp(D %*% b)))
}</code></pre>
<p>We can then create the cost function and put the logistic-function in the cost-function.</p>
<pre class="r"><code># Cost function for logistic regression
logistic_cost &lt;- function(y, D, b){
  y_hat &lt;- logistic_function(D, b)
  1/length(y_hat) * sum(y * -log(y_hat) + (1 - y) * -log((1 - y_hat)))
}</code></pre>
<p>To conduct the grid approximation, I write another function that takes the grid containing parameters, the predictor matrix and the outcome variable y as arguments and them computes the cost for every parameter combination. The function returns a matrix containing the grid and the corresponding cost.</p>
<pre class="r"><code># Compute the cost for different combinations of regression weights
# Returns the costs in a vector
grid_approx_logistic &lt;- function(grid, data, outcome) {
  cost &lt;- NULL
  for(i in seq(nrow(grid))) {
    betas &lt;- as.numeric(grid[i, ])
    betas &lt;- create_weight_matrix(betas)
    cost[i] &lt;- logistic_cost(y = outcome, D = data, b = betas)    
  }
  cost_grid &lt;- as.matrix(cbind(grid, cost))
  return(cost_grid)
}

# Create helper function to construct the weight matrix given a vector with proposed weights
# Used in the grid_approx_logistic function
create_weight_matrix &lt;- function(weights) {
  betas &lt;- matrix(weights, nrow = length(weights))
  return(betas)
}</code></pre>
<p>I will use the <code>mtcars</code> dataset and I choose to model the probability beeing an automatic car as a function of miles per hour. First I set the grid with plausible values (I got them from running the glm function in the first place).</p>
<p>After that I create the predictor matrix, containing the intercept and the <code>mpg</code> data, and subset the variable <code>am</code> as outcome <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code># create grid
grid &lt;- expand.grid(seq(-20, 0, length.out = 50), seq(0, 1, length.out = 50))

# set intercept
mtcars$intercept &lt;- 1
# subset prediction matrix
data &lt;- as.matrix(mtcars[, c(&quot;intercept&quot;, &quot;mpg&quot;)])
# subset dependent variable
y &lt;- mtcars$am</code></pre>
<p>Next I run the <code>grid_approx_logistic</code> function which computes the cost for every combination.</p>
<pre class="r"><code># compute cost for each parameter combination
cost &lt;- grid_approx_logistic(grid = grid, data = data, outcome = y)</code></pre>
<p>I visulize the result with a combination of a raster and contour plot to visualize the parameter combination with the hightes likelihood.</p>
<p>Furthermore I added the regression weights computed by the <code>glm</code> function.</p>
<pre class="r"><code># compute the logistic regression with base r function glm
model &lt;- glm(am ~ mpg, family = &quot;binomial&quot;, data = mtcars)  
coefs &lt;- as.data.frame(map(coef(model), list)) %&gt;% set_names(c(&quot;beta0&quot;, &quot;beta1&quot;))

# visualize the regession combinations vs the computed cost
as.data.frame(cost) %&gt;% 
  ggplot(aes(x = Var1, y = Var2, z = cost, color = cost, fill = cost)) +
  geom_raster(interpolate = FALSE) +
  geom_contour(bins = 30) + 
  geom_point(data = coefs, aes(x = beta0, y = beta1), size = 4, color = &quot;white&quot;, inherit.aes = F) +
  geom_label(data = coefs, aes(x = beta0, y = beta1), 
             label = TeX(&quot;$\\beta_0, \\beta_1 = (-6.6, 0.3)$&quot;),
             inherit.aes = F,
             nudge_x = 1, nudge_y = .05, size = 5) +
  theme_classic() +
  scale_fill_continuous(&quot;Cost-Function&quot;) +
  scale_color_continuous(&quot;Cost-Function&quot;) +
  xlab(TeX(&quot;$\\beta_0$ (Intercept)&quot;)) +
  ylab(TeX(&quot;$\\beta_1$ (Regressionsgewicht für Miles per Hour)&quot;)) +
  ggtitle(&quot;Minimizing the Likelihood Function&quot;, sub = &quot;Grid Approximation&quot;) +
  theme(text = element_text(family = &quot;Times&quot;),
        title = element_text(size = 14), 
        axis.text = element_text(size = 12))</code></pre>
<p><img src="/post/2018-10-14-exploring-the-cost-function-of-logistic-regression_files/figure-html/visualize-ml-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>We can see that the area with the hightest density, resp highest likelihood, correspondes with the computed coefficents.</p>
<p>However, we can also see, that there is a area corresponding to low values of the cost function which maybe leads to unstable estimates of the regression coeficients.</p>
<p>Since I started with a grid that mirrors the result of the glm function, grid approximation is often a bad approach to find the best parameter combination of a logistic model.</p>
<p>Other approaches use the gradient decent algorithm, which is computational much cheaper and therefore faster.</p>
