---
title: "Finalizing a Glmnet Model: An Example using Credit Data"
author: Jens Hüsers
date: '2019-01-19'
draft: false  
categories:
  - R
  - glmnet
  - machine learning
  - predictive analysis
tags:
  - machine learning
  - R
editor_options: 
  chunk_output_type: console
---



<div id="evaluating-a-models-hyperparameter" class="section level1">
<h1>Evaluating a models hyperparameter</h1>
<p>In machine learning we develop algorithms to map from predictive variables to an output variable which we wish to predict using unseen data. When building models we can use hyperparameters of the algorithms to increase model performance.</p>
<p>An example is lasso regression which, very briefly, puts a constraint on the models coefficient in order to decrease variance of model coefcients and coeficient selection.</p>
<p>To figure out the appropriate hyperparameters cross-validation is applied, where cross-validation is a method to simulate the models performance using test and a training sets. The training set is used to train the model and the predictive performance is evaluated using the test set also known as the hold-out set.</p>
<p>This procedure is done for each proposed hyperparameter and the hold-out metric is calculated. The hyperparameter suppling the best hold-out metric, e.g. hightest R-Squared is used to train the final model.</p>
<p>Before doing so, we will conduct model training using the lasso regression estimating the hyperparameter lambda, ruling the amount of contraint. We will use the <code>caret</code> package to train a model predicting the Miles per Gallon using different variables in the <code>mtcars</code> dataset.</p>
<pre class="r"><code>data &lt;- ISLR::Credit %&gt;% 
  select(-ID) %&gt;% 
  mutate(random_noise = rnorm(n = nrow(.), sd = 10))

y &lt;- data$Income
# creating a matrix with predictors
X &lt;- model.matrix(Income ~ ., data)

# Setup the training process 
# We will use 5-fold crossvalidation

# 10 fold cross validation
tr_ctrl &lt;- caret::trainControl(method = &quot;repeatedcv&quot;, repeats = 1, number = 3)
hyperparams &lt;- expand.grid(lambda = c(seq(0.1, 4, .1)), alpha = 1)

set.seed(124193)
cv_models &lt;- caret::train(x = X,
                          y = y,
                          family = &quot;gaussian&quot;,
                          metric = &quot;Rsquared&quot;,
                          method = &quot;glmnet&quot;,
                          trControl = tr_ctrl,
                          tuneGrid = hyperparams)

lambda &lt;- purrr::pluck(cv_models, &quot;results&quot;) %&gt;%
  select(lambda, Rsquared) %&gt;%
  filter(max(Rsquared) == Rsquared) %&gt;%
  pull(lambda)

plot(cv_models)</code></pre>
<p><img src="/post/2019-01-25-finalizing-glmnet-models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="finalizing-the-model" class="section level1">
<h1>Finalizing the model</h1>
<p>We see in the plot that the cross-validated Residual Mean Squared Error peaks at an alpha value of .5, this hyperparameter value should be used in our final model.</p>
<p>In the next section we will use the <code>glmnet</code> function from the <code>glmnet</code> packages which allows us to create a regression model with the specific alpha value.</p>
<pre class="r"><code># Setting alpha to 1 yielding lasso regression
# Setting the regularization parameter lambda to 0.5 which yielded the lowest RMSE in the cross-validation procedure
final_model &lt;- glmnet::glmnet(x = X, y = y, family = &quot;gaussian&quot;, alpha = 1, lambda = lambda)

# Get final model coefficients
tidy_final_model &lt;- broom::tidy(final_model)  

# extracting coeficients
coefs &lt;- tidy_final_model$estimate

# extracting coeficient names
vars &lt;- tidy_final_model$term

# getting data to predict
newX &lt;- as.matrix(X[, vars])

# predict
y_hat &lt;- newX %*% as.matrix(coefs, nrow = 6)

# R-Squared
r_sq &lt;- 1 - (sum((y_hat[, 1] - y)^2) / sum((y - mean(y))^2))
r_sq</code></pre>
<pre><code>## [1] 0.906947</code></pre>
<p>Following the hyperparameter tuning of lambda, the regularization parameter of the Lasso-Regression we eventually fitted the final model using the hyperparameter lambda which yielded the highest cross-validated R-Squared statistic; in this example the value of lambda was <span class="math inline">\(\lambda\)</span> = 0.1 with a final R-Square value of 0.906947, however we do not bother about this value, since we estimated the R-Squared statistic for unseen data using cross-validation.</p>
</div>
